"use strict";(self.webpackChunkchunggi_lee_homepage=self.webpackChunkchunggi_lee_homepage||[]).push([[597],{597:function(e,t,i){i.r(t),i.d(t,{default:function(){return W}});var n=i(7294),a=i(917),o=i(2658),r=i(8440),s=i(5343),l=i(131),c=i(5944);var d=({children:e,delay:t})=>{const i={visible:{opacity:1,transition:{duration:1,delay:void 0===t?0:t}},hidden:{opacity:0}},a=(0,r._)(),[o,d]=(0,l.YD)();return(0,n.useEffect)((()=>{d?a.start("visible"):a.start("hidden")}),[a,d]),(0,c.tZ)(s.E.div,{ref:o,animate:a,initial:"hidden",variants:i,children:e})};var h=({text:e})=>{const t="ABOUT"===e?"24%":"30%";return(0,c.BX)(d,{children:[(0,c.tZ)(o.Z,{variant:"h4",align:"center",children:e}),(0,c.tZ)("div",{css:(0,a.iv)("margin:0 auto;width:",t,";height:5px;background:#03c2c9;","")})]})},g=i.p+"457d02f89b5ca61cb838654a0fccb3f0.png",u=i(1508),p=i(5725),m=i(6634),f=i(8698),v=i(1671),w=i(5518);var y=({type:e,title:t,content:i})=>{const{isLight:n}=(0,w.T)();return(0,c.BX)(u.Z,{sx:{display:"flex",alignItems:"center",marginTop:2},children:["SCHOOL"===e?(0,c.tZ)(c.HY,{children:(0,c.tZ)(f.Z,{fontSize:"large"})}):(0,c.tZ)(v.Z,{fontSize:"large"}),(0,c.tZ)(u.Z,{sx:{display:"flex",alignItems:"left",flexDirection:"column"},children:(0,c.BX)(c.HY,{children:[(0,c.tZ)(o.Z,{marginLeft:1,fontWeight:500,variant:"h6",align:"left",children:t}),i.split("\n").map((e=>(console.log(e),(0,c.tZ)(o.Z,{marginLeft:1,color:n?"text.primary":"white",variant:"h6",align:"left",children:e}))))]})})]})};const b=[{title:"Ulsan National Institute of Science and Technology (UNIST)",content:"M.S in Computer Science and Engineering, 2018~2020"},{title:"Ulsan National Institute of Science and Technology (UNIST)",content:"B.S in Computer Science and Engineering, 2014~2018"}],Z=[{title:"Lunit",content:"Research Engineer, 2020~ \n (Alternative Military Service)"},{title:"iVADER Lab",content:"Researcher, 2017~2020"}],x=({text:e,size:t})=>(0,c.tZ)(o.Z,{align:"center",variant:t,children:e});var k={name:"d11srq",styles:"margin-top:80px"};var T=(0,n.forwardRef)(((e,t)=>(0,c.BX)("div",{ref:t,children:[(0,c.tZ)("div",{css:k}),(0,c.tZ)(h,{text:"ABOUT",width:"12%"}),(0,c.tZ)("br",{}),(0,c.tZ)(u.Z,{sx:{display:"flex",p:1,m:1,borderRadius:1},children:(0,c.BX)(p.ZP,{container:!0,style:{flexDirection:"row-reverse",justifyContent:"center"},spacing:3,children:[(0,c.tZ)(p.ZP,{item:!0,xs:3,style:{justifyContent:"center"},children:(0,c.BX)(d,{delay:.3,children:[(0,c.tZ)("img",{style:{borderRadius:10,objectFit:"cover",objectPosition:"-20% -50%",width:"100%"},src:g}),(0,c.tZ)(x,{size:"h4",text:"Chunggi Lee"}),(0,c.tZ)(x,{size:"h5",text:"Lunit"}),(0,c.tZ)(x,{size:"h5",text:"Research Engineer"})]})}),(0,c.BX)(p.ZP,{item:!0,lg:!0,children:[(0,c.tZ)(d,{delay:.3,children:(0,c.BX)(o.Z,{variant:"h5",children:["Hello! I am Chunggi Lee, a Research Engineer at"," ",(0,c.tZ)(m.Z,{target:"_blank",href:"https://www.lunit.io",children:"Lunit"}),". My work is to design and build interactive systems and techniques to gain more high quality pathology data and make more accurate deep learning models in cell and tissue. I am also interested in visual analytics and human-computer interaction, especially making interactive tools and techniques, and data-driven applications with deep learning. I received B.S. and M.S. degrees in the computer science department at"," ",(0,c.tZ)(m.Z,{target:"_blank",href:"https://www.unist.ac.kr",children:"Ulsan National Institute of Science and Technology (UNIST)"}),". I worked with Sungahn Ko in the"," ",(0,c.tZ)(m.Z,{target:"_blank",href:"https://ivader.unist.ac.kr/",children:"Interactive Visual Analysis and Data Exploration Research (iVADER) Lab"}),"."]})}),(0,c.BX)(u.Z,{sx:{display:"flex",justifyContent:"space-between"},children:[(0,c.tZ)(d,{delay:.6,children:(0,c.BX)(u.Z,{children:[(0,c.tZ)(o.Z,{marginTop:2,variant:"h4",children:"Education"}),b.map((e=>(0,c.tZ)(y,{type:"SCHOOL",title:e.title,content:e.content})))]})}),(0,c.tZ)(d,{delay:.6,children:(0,c.BX)(u.Z,{children:[(0,c.tZ)(o.Z,{marginTop:2,variant:"h4",children:"Work Experiences"}),Z.map((e=>(0,c.tZ)(y,{type:"COMPANY",title:e.title,content:e.content})))]})})]})]})]})})]}))),C=i(8692),S=i(4962),I=i(2643);const A="Chunggi Lee",B=(e,t,i,n)=>{const a=e.split(A);return(0,c.tZ)(u.Z,{sx:{display:"flex"},children:(0,c.BX)(o.Z,{variant:"h6",color:n,children:[a[0],(0,c.tZ)(u.Z,{sx:{borderBottom:t?"2px solid black":"2px solid #03c2c9"},color:i,fontWeight:500,display:"inline",children:A}),a[1]]})})};var D=({media:e,title:t,author:i,conference:n,tags:a})=>{const{isLight:r}=(0,w.T)(),l=r?"text.primary":"white",h=r?"text.secondary":"white",g=!0===r?"white":"#525252";return(0,c.tZ)(d,{children:(0,c.tZ)(C.Z,{sx:{width:"100%",marginTop:1,marginBottom:1,backgroundColor:g},children:(0,c.BX)(u.Z,{sx:{display:"flex",p:1,m:1,borderRadius:1,flexDirection:{xs:"column",md:"row"}},children:[(0,c.tZ)(s.E.button,{animate:{backgroundColor:"white",border:"none"},whileHover:{scale:1.1},whileTap:{scale:1},children:(0,c.tZ)(S.Z,{sx:{width:{md:"30vh",xs:"100%"}},component:"img",image:e,alt:t})}),(0,c.BX)(I.Z,{children:[(0,c.tZ)(o.Z,{color:l,fontWeight:600,variant:"h5",children:t}),B(i,r,l,h),(0,c.tZ)(o.Z,{variant:"h6",color:h,children:n}),(0,c.tZ)(u.Z,{sx:{display:"flex"},children:a.map((e=>(0,c.tZ)(o.Z,{marginRight:1,variant:"h6",children:(0,c.tZ)(m.Z,{href:e.link,children:e.tag})})))})]})]})})})},E=i.p+"8419edd374824ad46fbe8376fc8df6e1.png",P=i.p+"0d1636b5ad002128c0c7fe72ae37f4af.png",R=i.p+"69d46770bcdf33a25e958ad1821644f7.png",O=i.p+"9bee46a290566b297d150d084bfecc79.png",V=i.p+"27e94bfcd4868481470dfcc6541af1e5.png",L=i.p+"9ef08de61b2edce8531e93a94786019c.png",U=i.p+"TVCG2019.pdf";var F=[{title:"Interactive Multi-Class Tiny-Object Detection.",author:"Chunggi Lee, Seonwook Park, Heon Song, Jeongun Ryu, Sanghoon Kim, Haejoon Kim, Sergio Pereira, Donggeun Yoo",conference:"IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022, Accepted ",image:E,tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/2203.15266"},{tag:"[Video]",link:"https://vimeo.com/700148797"}]},{title:"GUIComp: A GUI Design Assistant with Real-Time, Multi-Faceted Feedback.",author:"Chunggi Lee, Sanghoon Kim, Dongyun Han, Hongjun Yang, Young-Woo Park, Bum Chul Kwon, Sungahn Ko",conference:"ACM CHI Conference on Human Factors in Computing Systems (CHI), 2020, Accepted",image:P,tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/2001.05684"},{tag:"[Preview Video]",link:"https://www.youtube.com/watch?v=UkqTStZEVbo"},{tag:"[Video]",link:"https://vimeo.com/700148306"}]},{title:"STGRAT: A Spatio-Temporal Graph Attention Network for Traffic Forecasting",author:"Cheonbok Park,  Chunggi Lee, Hyojin Bahng, Taeyun Won, Kihwan Kim, Seungmin Jin, Sungahn Ko, Jaegul Choo",conference:"ACM International Conference on Information and Knowledge Management (CIKM), 2020, Accepted",image:R,tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/1911.13181"}]},{title:"A visual analytics system for exploring, monitoring, and forecasting road traffic congestion.",author:"Chunggi Lee, Yeonjun Kim, Seungmin Jin, Dongmin Kim, Ross Maciejewski, David Ebert, and Sungahn Ko",conference:"IEEE transactions on visualization and computer graphics (TVCG IF=4.579), 2019, Accepted. Invited (Proc. IEEE VIS`19)",image:O,tags:[{tag:"[PDF]",link:U},{tag:"[Video]",link:"https://vimeo.com/700148275"}]},{title:"An Empirical Study on the Relationship Between the Number of Coordinated Views and Visual Analysis.",author:"Juyoung Oh, Chunggi Lee, Hwiyeon Kim, Kihwan Kim, Osang Kwon, Eric D. Ragan, Bum Chul Kwon, Sungahn Ko",conference:"Arxiv, 2018",image:V,tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/2204.09524"}]},{title:"A Graphical Workflow Exploration Environment For Visual Analytics.",author:"Chunggi Lee, Juyoung Oh, Seungmin Jin, Isaac Cho, and Sungahn Ko",conference:"Arxiv, 2018",image:L,tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/2204.10221"},{tag:"[Video]",link:"https://vimeo.com/700148330"}]}];var G=(0,n.forwardRef)(((e,t)=>(0,c.BX)("div",{ref:t,children:[(0,c.tZ)(h,{text:"PUBLICATIONS",width:"25%"}),(0,c.tZ)("br",{}),(0,c.tZ)(u.Z,{sx:{display:"flex",flexDirection:"column",p:1,m:1,borderRadius:1},children:F.map((e=>(0,c.tZ)(D,{media:e.image,author:e.author,conference:e.conference,title:e.title,tags:e.tags})))})]}))),j=i(4567),X=i(7641),K=i(7601);const M={position:"absolute",top:"50%",left:"50%",transform:"translate(-50%, -50%)",width:{xs:"90%",md:"70%"},border:"2px solid #000",boxShadow:24,p:{xs:2,md:4},overflowY:"scroll",height:"100%",display:"flex",alignItems:"center",flexDirection:"column"};function H({open:e,setOpen:t,title:i,img:n,content:a,tags:r}){const{isLight:s}=(0,w.T)();return(0,c.tZ)("div",{children:(0,c.tZ)(X.Z,{"aria-labelledby":"transition-modal-title","aria-describedby":"transition-modal-description",open:e,onClose:()=>(console.log(e),void t(!1)),closeAfterTransition:!0,BackdropComponent:j.Z,BackdropProps:{timeout:500},children:(0,c.tZ)(K.Z,{in:e,children:(0,c.BX)(u.Z,{sx:M,bgcolor:s?"white":"#525252",children:[(0,c.tZ)(o.Z,{id:"transition-modal-title",variant:"h4",align:"center",children:i}),(0,c.tZ)(u.Z,{component:"img",sx:{width:{xs:"100%",md:"80%"}},src:n,alt:i,loading:"lazy"}),(0,c.tZ)(o.Z,{id:"transition-modal-description",variant:"h5",align:"center",children:"Abstract"}),(0,c.tZ)(o.Z,{id:"transition-modal-description",variant:"h6",sx:{mt:2},children:a}),(0,c.tZ)(u.Z,{sx:{display:"flex",alignItems:"center",flexDirection:"row"},children:r.map((e=>(0,c.tZ)(o.Z,{marginLeft:1,component:"a",href:e.link,color:"#03c2c9",variant:"h5",align:"center",children:e.tag})))})]})})})})}var z=({media:e,title:t,setOpen:i,idx:a,setSelectedIdx:r})=>{const[l,d]=n.useState(!1);return(0,c.tZ)(C.Z,{sx:{width:"100%",height:"20vh"},children:(0,c.BX)(u.Z,{onMouseEnter:()=>d(!0),onMouseLeave:()=>d(!1),sx:{position:"relative",height:"100%",display:"flex",alignItems:"center"},children:[(0,c.tZ)(S.Z,{component:"img",image:e,alt:t}),(0,c.tZ)(u.Z,{sx:{position:"absolute",bottom:0,left:0,width:"100%",height:"100%",bgcolor:"rgba(3, 194, 201, 1)",color:"white",opacity:0,"&:hover":{transition:".5s ease",opacity:1},display:"flex",alignItems:"center",justifyContent:"center",flexDirection:"column"},children:l?(0,c.BX)(c.HY,{children:[(0,c.tZ)(s.E.div,{initial:{y:"-200%"},animate:{y:"0%"},transition:{duration:.5},children:(0,c.tZ)(o.Z,{variant:"h6",align:"center",children:t})}),(0,c.tZ)(s.E.div,{initial:{y:"200%"},animate:{y:"0%"},transition:{duration:.5},children:(0,c.tZ)(u.Z,{sx:{display:"flex",alignItems:"center",justifyContent:"center"},children:(0,c.tZ)(u.Z,{children:(0,c.tZ)(o.Z,{sx:{boder:1},onClick:e=>{"modalButton"==e.target.id&&(i(!0),r(a))},color:"#FF69B4",fontWeight:500,variant:"h5",align:"center",id:"modalButton",children:(0,c.tZ)(c.HY,{children:"[Detail]"})})})})})]}):(0,c.tZ)(c.HY,{})})]})})};var N=[{title:"Interactive Multi-Class Tiny-Object Detection.",image:E,content:"Annotating tens or hundreds of tiny objects in a given     image is laborious yet crucial for a multitude of Computer     Vision tasks. Such imagery typically contains objects from     various categories, yet the multi-class interactive annotation     setting for the detection task has thus far been unexplored.     To address these needs, we propose a novel interactive annotation     method for multiple instances of tiny objects     from multiple classes, based on a few point-based user inputs.     Our approach, C3Det, relates the full image context     with annotator inputs in a local and global manner via latefusion and     feature-correlation, respectively. We perform experiments on the Tiny-DOTA and LCell datasets using both     two-stage and one-stage object detection architectures to     verify the efficacy of our approach. Our approach outperforms existing approaches     in interactive annotation, achieving higher mAP with fewer clicks. Furthermore, we validate     the annotation efficiency of our approach in a user study     where it is shown to be 2.85x faster and yield only 0.36x     task load (NASA-TLX, lower is better) compared to manual     annotation. The code is available at https://github.     com/ChungYi347/Interactive-Multi-ClassTiny-Object-Detection. ",tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/2203.15266"},{tag:"[Video]",link:"https://vimeo.com/700148797"}]},{title:"Interactive Segmentation for Tissue.",image:i.p+"3034f015e48ebc6c0dab4d2c6e9d714c.png",content:"Object segmentation play an important role in the workflow of computational pathlogy.     Unlike general object segmentation, manually labeling pathology data is very time consuming and     expensive due to requiring expert knowledge. One solution is that interactive segmentaiton method     offer more effient by using user-provided inputs such as points, lines, and polygon, which means      user gives information of classes models and model automatically find similar areas.     We build a new novel method which has a undertanding given user inputs with user input enforcing     loss based on the DeepLab V3+ model. On top of that, we generated random points or many diverse curved-lines to     simulate user inputs. In our real-annotation process, we proved that this method takes 14.7% of     time compared to manuall annotation. The procudure is above figure 1) draw points, lines, and polygons     2) request inference and check the results. 3) revised given user inputs and request inference     , and 4) check whether the annotation is correct or not.",tags:[{tag:"[Video]",link:"https://vimeo.com/700149891"}]},{title:"GUIComp: A GUI Design Assistant with Real-Time, Multi-Faceted Feedback.",image:P,content:"Users may face challenges while designing graphical user     interfaces, due to a lack of relevant experience and guidance.     This paper aims to investigate the issues that users with no     experience face during the design process, and how to resolve     them. To this end, we conducted semi-structured interviews,     based on which we built a GUI prototyping assistance tool     called GUIComp. This tool can be connected to GUI design     software as an extension, and it provides real-time, multifaceted     feedback on a user’s current design. Additionally, we     conducted two user studies, in which we asked participants to     create mobile GUIs with or without GUIComp, and requested     online workers to assess the created GUIs. The experimental     results show that GUIComp facilitated iterative design and the     participants with GUIComp had better a user experience and     produced more acceptable designs than those who did not.",tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/2001.05684"},{tag:"[Preview Video]",link:"https://www.youtube.com/watch?v=UkqTStZEVbo"},{tag:"[Video]",link:"https://vimeo.com/700148306"}]},{title:"STGRAT: A Spatio-Temporal Graph Attention Network for Traffic Forecasting",image:R,content:"Predicting road traffic speed is a challenging task due to different      types of roads, abrupt speed change and spatial dependencies     between roads; it requires the modeling of dynamically changing     spatial dependencies among roads and temporal patterns over long     input sequences. This paper proposes a novel spatio-temporal graph     attention (ST-GRAT) that effectively captures the spatio-temporal     dynamics in road networks. The novel aspects of our approach     mainly include spatial attention, temporal attention, and spatial     sentinel vectors. The spatial attention takes the graph structure     information (e.g., distance between roads) and dynamically adjusts     spatial correlation based on road states. The temporal attention is     responsible for capturing traffic speed changes, and the sentinel     vectors allow the model to retrieve new features from spatially     correlated nodes or preserve existing features. The experimental     results show that ST-GRAT outperforms existing models, especially     in difficult conditions where traffic speeds rapidly change (e.g., rush     hours). We additionally provide a qualitative study to analyze when     and where ST-GRAT tended to make accurate predictions during     rush-hour times.",tags:[{tag:"[PDF]",link:"https://arxiv.org/abs/1911.13181"}]},{title:"A visual analytics system for exploring, monitoring, and forecasting road traffic congestion.",image:O,content:"We present an interactive visual analytics system that     enables traffic congestion exploration, surveillance, and forecasting     based on vehicle detector data. Through domain expert collaboration,     we have extracted task requirements, incorporated the Long     Short-Term Memory (LSTM) model for congestion forecasting, and     designed a weighting method for detecting the causes of     congestion and congestion propagation directions. Our visual     analytics system is designed to enable users to explore congestion     causes, directions, and severity. Congestion conditions of a city are     visualized using a Volume-Speed Rivers (VSRivers) visualization     that simultaneously presents traffic volumes and speeds. To evaluate     our system, we report performance comparison results, wherein     our model is more accurate than other forecasting algorithms. We     demonstrate the usefulness of our system in the traffic management     and congestion broadcasting domains through three case studies and     domain expert feedback.",tags:[{tag:"[PDF]",link:U},{tag:"[Link]",link:'"https://ieeexplore.ieee.org/document/8735916"'},{tag:"[Video]",link:"https://vimeo.com/700148275"}]},{title:"A Traffic Forecasting System.",image:i.p+"93c837f04aad65119351abefe2612b74.png",content:"Intelligent transportation such sensors under ground has been made our life more conenient and efficent.      It leads to giving tremendous data us to develop traffic forecasting models.      By using these tremendous data, deep learning models (e.g., LSTM, CNN, and GNN) have been applied traffic prediction      to learn spatial and temporal dynamics. In this project, we built a traffic forecasting system to make broadcasting scripts      during traffic broadcasting, providing detour routes with forecasting listeners. This detour routes have been solved traffic congestion      during rush hours. We have deployed these systems in metropolitan cities like Busan, Ulsan, and etc.      On top of that, this system visualizes current traffic situaion and CCTV views of each individual road.",tags:[]},{title:"The Relationship between the Number of Coordinated Views and Visual Data Analysis.",image:V,content:"Many visualization applications have been developed using multiple coordinated views.     Our widely, safely adopted assumption is that many views bring forth fruitful outcomes,     such as new perspectives, various analytic paths, and accurate results.     However, very little empirical work has been reported to prove or disprove these beliefs.     Our study aims to investigate the relationship between the number of coordinated views     and users analytic processes and results. Furthermore, we aim to provide a design     guideline on how to help users create and manage more views if having more views is     indeed more helpful. To achieve the goals, we     conducted a human subject study with 44 participants. In the experiment, we asked     participants to solve five analytic problems using     a visualization system. Through quantitative and qualitative analysis,     we discovered the positive correlation between the number of     views and analytic results. We also found that a simple view creation     technique called dynamic visualization cloning (DVC) encourage     users to create more views and to take a variety of analytic paths.     Based on the results, we provide implications and limitations of our study.",tags:[{tag:"[PDF]",link:""}]},{title:"A Visual Analysis System for Gene Expression Analysis.",image:L,content:"Graphical history mechanisms have been widely utilized in     many domains to support humans’ limited working memory,     error recovery, collaboration, and presentation in visual analysis.     Yet, there are aspects that remain under-explored in designing     graphical history systems for visual analytics systems to help     analysts who have complicated workflows. In this paper we report     on our design study performed with domain experts, where we     characterize domain tasks and designed a visual graphical workflow     management environment. Our environment allows analysts to efficiently     review, edit, navigate, and explore their complex workflows     with their colleagues. In order to evaluate the environment,     we present a case study and user study. In the case study, we explore     how two domain experts perform collaborative review, communication,     and training with our environment; while in the user study with     the car data, we reveal that how our environment helps users and     how the history mechanism affects users’ visual problem-solving behaviors",tags:[{tag:"[PDF]",link:""},{tag:"[Video]",link:"https://vimeo.com/700148330"}]},{title:"Circos for Exploring Relationships between Genes.",image:i.p+"6f594195d95dc9d9eab224afd4c00958.png",content:"Circos is a visualization method to shows differences and     similiarities among genomes. We developed a visualization tool with     circos to faciliate relationships between pairs of positions of genes.",tags:[]}];var q=(0,n.forwardRef)(((e,t)=>{const[i,a]=(0,n.useState)(0),{open:o,setOpen:r}=(0,w.T)();return(0,c.BX)("div",{ref:t,children:[(0,c.tZ)(h,{text:"PROJECTS",width:"12%"}),(0,c.tZ)("br",{}),(0,c.BX)(p.ZP,{sx:{display:"flex",flexDirection:{xs:"row",sm:"row"},p:1,borderRadius:1,flexWrap:"wrap"},container:!0,spacing:1,children:[N.map(((e,t)=>{const i=.2*parseInt(t/2);return(0,c.tZ)(p.ZP,{item:!0,xs:6,sm:3,md:3,children:(0,c.tZ)(d,{delay:i,children:(0,c.tZ)(z,{media:e.image,title:e.title,setOpen:r,idx:t,setSelectedIdx:a})})})})),(0,c.tZ)(H,{open:o,setOpen:r,title:N[i].title,img:N[i].image,content:N[i].content,tags:N[i].tags})]})]})}));var W=()=>{const{refs:e}=(0,w.T)();return(0,c.BX)(c.HY,{children:[(0,c.tZ)(T,{ref:e.ABOUT}),(0,c.tZ)(G,{ref:e.PUBLICATIONS}),(0,c.tZ)(q,{ref:e.PROJECTS})]})}}}]);